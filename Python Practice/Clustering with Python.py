# -*- coding: utf-8 -*-
"""Muhammad Esza Maulana Firmanda - Tugas Clustering with Python.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ECdhTc1Lmi08uVM9b2p30bc7vXOERoZ7
"""

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.neighbors import KNeighborsClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
import xgboost as xgb
from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score

from google.colab import drive
drive.mount('/content/gdrive')

data = pd.read_csv('/content/gdrive/MyDrive/SIB Greatedu praktik/titanic_dataset/titanic-train.csv')
data.set_index("PassengerId", inplace=True)

data

data.describe()

# mengisi data kosong pada kolom Age
data['Age'].fillna(data['Age'].median(), inplace=True)

# mengubah kolom berisi kategori menjadi variabel yang terpisah tiap kategori
data = pd.get_dummies(data, columns=['Sex', 'Embarked'])

# Menampilkan kolom nama yang diubah saat encoding
encoded_columns = data.columns
print(encoded_columns)

"""Disini, permasalahannya adalah,

apakah ada pengaruh antara variabel X yang berupa kolom yang berisi:
jenis kelamin, kelas 1 atau 2 dan 3, jumlah saudara atau pasangan yang ikut naik, jumlah orang tua atau anak yang ikut naik, tarif penumpang dan pelabuhan tempat pemberangkatan (C = Cherbourg, Q = Queenstown, S = Southampton),

terhadap variabel target (y) yang berisi selamat atau tidaknya penumpang

kolom cabin tidak saya gunakan karena terlalu banyak missing value dan memiliki banyak value yang unik,

sementara kolom tiket tidak saya gunakan karena saya anggap tidak signifikan terhadap variabel target, juga setiap penumpang memiliki nomor tiket yang berbeda
"""

# Variabel independen/target (y) dan variabel dependen(X)
X = data[['Pclass', 'Age', 'SibSp', 'Parch', 'Fare', 'Sex_female', 'Sex_male', 'Embarked_C', 'Embarked_Q', 'Embarked_S']]
y = data['Survived']

# memisahkan the dataset ke training dan testing
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)

X_test

y_test

"""Menggunakan Classifier"""

# K-Nearest Neighbors (KNN)
knn = KNeighborsClassifier(n_neighbors=5)
knn.fit(X_train, y_train)
knn_pred = knn.predict(X_test)

# Decision Tree
dt = DecisionTreeClassifier()
dt.fit(X_train, y_train)
dt_pred = dt.predict(X_test)

# Random Forest
rf = RandomForestClassifier()
rf.fit(X_train, y_train)
rf_pred = rf.predict(X_test)

# XGBoost
xgb_model = xgb.XGBClassifier()
xgb_model.fit(X_train, y_train)
xgb_pred = xgb_model.predict(X_test)

"""Menampilkan prediksi dari tiap teknik classifier"""

actual = y_test  # Actual target values
# Create confusion matrices
knn_cm = confusion_matrix(actual, knn_pred)
dt_cm = confusion_matrix(actual, dt_pred)
rf_cm = confusion_matrix(actual, rf_pred)
xgb_cm = confusion_matrix(actual, xgb_pred)

# Convert confusion matrices to DataFrames
def confusion_matrix_to_dataframe(cm):
    labels = sorted(set(actual))
    df_cm = pd.DataFrame(cm, index=labels, columns=labels)
    df_cm.index.name = 'Actual'
    df_cm.columns.name = 'Predicted'
    return df_cm

# Convert confusion matrices to DataFrames with labels
knn_cm_df = confusion_matrix_to_dataframe(knn_cm)
dt_cm_df = confusion_matrix_to_dataframe(dt_cm)
rf_cm_df = confusion_matrix_to_dataframe(rf_cm)
xgb_cm_df = confusion_matrix_to_dataframe(xgb_cm)

knn_cm_df

dt_cm_df

rf_cm_df

xgb_cm_df

"""Mengukur akurasi dari tiap Teknik Classifier"""

def evaluate(y_true, y_pred):
    cm = confusion_matrix(y_true, y_pred)
    accuracy = accuracy_score(y_true, y_pred)
    precision = precision_score(y_true, y_pred, average='weighted')
    recall = recall_score(y_true, y_pred, average='weighted')
    f1 = f1_score(y_true, y_pred, average='weighted')
    return cm, accuracy, precision, recall, f1


# Evaluate KNN
knn_cm, knn_accuracy, knn_precision, knn_recall, knn_f1 = evaluate(y_test, knn_pred)

# Evaluate Decision Tree
dt_cm, dt_accuracy, dt_precision, dt_recall, dt_f1 = evaluate(y_test, dt_pred)

# Evaluate Random Forest
rf_cm, rf_accuracy, rf_precision, rf_recall, rf_f1 = evaluate(y_test, rf_pred)

# Evaluate XGBoost
xgb_cm, xgb_accuracy, xgb_precision, xgb_recall, xgb_f1 = evaluate(y_test, xgb_pred)

print('akurasi knn', knn_f1)
print('akurasi dt', dt_f1)
print('akurasi rf', rf_f1)
print('akurasi xgb', xgb_f1)

"""Dari output f1 diatas, didapatkan bahwa teknik Random forest memiliki akurasi yang paling tinggi dengan angka 0.79

diurutkan dari teknik yang paling akurat:
1. Random Forest (79.9)
2. XGBoost (78.9)
3. Decision Tree (73.5)
4. KNN (70)

Mengukur signifikansi atau pengaruh dari variabel x terhadap variabel target (y)
"""

# Buat objek RandomForestClassifier
classifier = RandomForestClassifier()

# Latih model pada data train
classifier.fit(X_train, y_train)

# Dapatkan feature importance
feature_importance = classifier.feature_importances_

# Buat DataFrame untuk lebih mudah membaca
importance_df = pd.DataFrame({'Feature': X_train.columns, 'Importance': feature_importance})

# Urutkan berdasarkan kepentingan (importance) secara menurun
importance_df = importance_df.sort_values(by='Importance', ascending=False)

# Tampilkan hasil
print(importance_df)

"""Dari hasil output diatas, "Feature Importance" mengindikasikan seberapa pentingnya masing-masing fitur dalam model Random Forest yang digunakan.

Semakin tinggi nilai "Feature Importance," semakin signifikan fitur tersebut dalam mempengaruhi prediksi model.

Signifikansi fitur-fitur tersebut dari yang paling signifikan ke yang paling kurang signifikan antara lain:

1. **Fare (0.262064):** Fitur Fare memiliki "Feature Importance" tertinggi, sehingga dapat dianggap sebagai fitur yang paling signifikan dalam memprediksi kelangsungan hidup. kemungkinan menunjukkan bahwa penumpang dengan tiket mahal atau kelas atas diprioritaskan dalam evakuasi.

2. **Age (0.250869):** Fitur Age juga memiliki "Feature Importance" yang tinggi, menunjukkan bahwa usia penumpang memiliki dampak signifikan terhadap prediksi. Ini bisa berarti bahwa usia memainkan peran penting dalam mempengaruhi selamat atau tidaknya seseoran, mungkin penumpang dengan usia lebih muda diprioritaskan sehingga tingkat keselamatan lebih tinggi.

3. **Sex_female (0.154480):** Ini adalah fitur yang mengindikasikan jenis kelamin sebagai perempuan. Karena memiliki "Feature Importance" yang tinggi, jenis kelamin perempuan dapat dianggap sebagai faktor penting dalam memprediksi kelangsungan hidup. Ini menunjukkan bahwa wanita mungkin mungkin lebih diprioritaskan untuk selamat dalam insiden Titanic.

4. **Sex_male (0.128270):** Sebagai laki-laki, meskipun "Feature Importance" nya lebih rendah dibandingkan dengan "Sex_female", laki-laki tetap memiliki dampak yang signifikan dalam prediksi.

5. **Pclass (0.084398):** Fitur Pclass, yang mengindikasikan kelas tiket, juga memiliki dampak yang cukup besar dalam prediksi. Ini mungkin menunjukkan bahwa kelas tiket berhubungan dengan peluang selamat dan berhubungan dengan variabel Fare.

6. **SibSp (0.046523):** Jumlah saudara atau pasangan (SibSp) juga memiliki dampak, meskipun lebih rendah dibandingkan dengan fitur-fitur sebelumnya. menunjukkan bahwa apakah seseorang memiliki saudara atau pasangan di kapal dapat mempengaruhi prediksi.

7. **Parch (0.036135):** Jumlah orang tua atau anak-anak (Parch) memiliki dampak yang lebih rendah dibandingkan dengan fitur lainnya, tetapi masih signifikan. menunjukkan bahwa apakah seseorang memiliki orang tua atau anak-anak di kapal juga memainkan peran dalam prediksi.

8. **Embarked_S (0.014869):** Fitur Embarked_S, yang mengindikasikan pelabuhan keberangkatan sebagai Southampton, memiliki dampak yang lebih rendah. Sepertinya pelabuhan keberangkatan tidak memiliki dampak besar pada prediksi, dan karena Importance yang didapat kurang dari 0.025, saya tidak akan menyertakannya kedalam prediksi.

9. **Embarked_C (0.014266):** Fitur Embarked_C, yang mengindikasikan pelabuhan keberangkatan sebagai Cherbourg, serupa dengan Embarked_S

10. **Embarked_Q (0.008125):** Fitur Embarked_Q, yang mengindikasikan pelabuhan keberangkatan sebagai Queenstown, memiliki dampak paling rendah dalam prediksi, sehingga dianggap tidak memiliki dampak.

PREDIKSI PENUMPANG YANG SELAMAT DARI TITANIC DATA TEST CSV
"""

test_data = pd.read_csv('/content/gdrive/MyDrive/SIB Greatedu praktik/titanic_dataset/titanic-test.csv')
test_data.set_index('PassengerId', inplace=True)
test_data.head()

# mengisi data kosong pada kolom Age
test_data['Age'].fillna(test_data['Age'].median(), inplace=True)

# mengubah kolom berisi kategori menjadi variabel yang terpisah tiap kategori
test_data = pd.get_dummies(test_data, columns=['Sex', 'Embarked'])

# Menampilkan kolom nama yang diubah saat encoding
encoded_columns = data.columns
print(encoded_columns)

y = data["Survived"]

features = ['Pclass', 'Age', 'SibSp', 'Parch', 'Fare', 'Sex_female', 'Sex_male']
X = pd.get_dummies(data[features])

# memastikan konsistensi kolom di test data
X_test = pd.get_dummies(test_data[features])

# mengganti missing data
X_test.fillna(0, inplace=True)

# model prediksi
model = RandomForestClassifier()
model.fit(X, y)

# Membuat prediksi
predictions = model.predict(X_test)

# Membuat output dataframe untuk prediksi
output = pd.DataFrame({"PassengerId": X_test.index, 'Survived': predictions})

output